\section{Chapter 4}
\subsection{Continuous random variables and their distributions}
Suppose we have an interval on the real line, $[a,b]$ with a uniformly distributed probability of a given value being picked, we know that the probability of each point $P(X=x)=0$ as there is an infinite amount of points, as such, it only makes sense to look at subintervals of the interval when it comes to probability. 

Simply from the definition of a CDF we have that $F_{X}(x<a)=0$, whilst $F_{X}(x\geq b)=1$, simultaneously we can establish a general equation for the probability of an interval as the proportion it constitutes of the total interval
\begin{align*}
    F_{X}(a\leq x_{1}\leq x_{2}\leq b)&=P(X\in[x_{1},x_{2}]) \\
                       &=\frac{x_{2}-x_{1}}{b-a}
\end{align*}
Using this definition we can create a CDF as
\[
    F_{X}(x)\begin{cases}0 & x<a \\ \frac{x-a}{b-a} & a\leq x\leq b \\ 1 & x\geq b\end{cases}
    
\]
Whether we use $<$ or $\leq$ (or the reverse), doesn't matter as the probability of each individual point is equal to 0, as such $P(X<2)=P(X\leq 2)$.
\begin{definition}
  A random variable $X$ with CDF $F_{X}(x)$ is said to be continuous if $F_{X}(x)$ is a continuous function for all $x\in \mathbb{R}$.
\end{definition}
\subsection{Probability density function}
As its impossible to define a PMF for a continuous function (as $P(X=x)=0$ for all $x\in \mathbb{R}$), as instead define the probability density function as 
\[
    f_{X}(x)=\lim_{\Delta\rightarrow0^{+}}\frac{P(x<X\leq x+\Delta)}{\Delta}
\]
We recall that $P(a<X\leq b)=F_{X}(b)-F_{X}(a)$, as such we rewrite the limit as
\[
    f_{X}(x)=\lim_{\Delta\rightarrow0^{+}}\frac{F_{X}(x+\Delta)-F_{X}(x)}{\Delta}
\]
We then recognize this as the definition of the derivate, as such we can write that
\[
    f_{X}(x)&=\lim_{\Delta\rightarrow0^{+}}\frac{F_{X}(x+\Delta)-F_{X}(x)}{\Delta}=F'_{X}(x)
\]
\begin{definition}
  Consider a continuous random variable $X$ with an absolutely continuous CDF $F_{X}(x)$. The function $f_{X}(x)$ defined by
  \[
  f_{X}(x)=\frac{d}{dx}F_{X}(x)=F'_{X}(x)$ if $F_{X}(x) 
  \]
  is differentiable at $x$ is called the probability density function of $X$.
\end{definition}
Using the general example from the previous section we determine the PDF as
\begin{align*}
    F'_{X}(x)&=\frac{d}{dx}\frac{x-a}{b-a} \\
             &=\frac{1}{b-a}\left(\frac{d}{dx}x+\frac{d}{dx}-a\right) \\
             &=\frac{1}{b-a}
\end{align*}
As such we can define the probability density function as
\[
    f_{X}(X)=\begin{cases}\frac{1}{b-a} & a<x<b \\ 0 & x\notin [a,b]\end{cases}
\]
As the PDF is the derivative of the CDF, we can integrate a segment of the interval and get the probability thereof assuming it is absolutely continuous in that interval
\[
    F_{X}(x)=\int_{-\infty}^{x}f_{X}(x)dx
\]
At the same time the integral over the entire real line must be equal to $1$ in accordance with the axioms of probability
\[
    \int_{-\infty}^{\infty}f_{X}(x)dx=1
\]
Whilst we can determine the probability of an interval as
\[
    P(a<X\leq b)=F_{X}(b)-F_{X}(a)=\int_{a}^{b}f_{X}(x)dx
\]
Similarly we can define the range of a random variable $X$ as the possible values of the random varibles where the PDF is larger than $0$, as such
\[
    R_{X}=\{x|f_{X}(x)>0\}
\]
\subsection{Expected value and variance}
In discrete random variables we frequently used sums to determine different values, in the case of continuous random variables we replace the sum with an integral sign and the PMF with the PDF, using LOTUS as an example we for example get that
\begin{align*}
    EX_{discrete}&=\sum_{x_{k}\in R_{X}}x_{k}P_{X}(x_{k}) \\
    EX_{continuous}&=\int_{-\infty}^{\infty}xf_{X}(x)dx
\end{align*}
For variance we can do the same as
\begin{align*}
    \text{Var}(X)&=E[(X-\mu_{X})^{2}]\\
             &=EX^{2}-(EX)^{2} \\
             &=\int_{-\infty}^{\infty}x^{2}f_{X}(x)dx-\mu_{X}^{2}
\end{align*}
\subsection{Functions of continuous random variables}
If $X$ is a continuous random variable and $Y=g(X)$ is a function of $X$, then $Y$ is a random variable, as such finding the CDF and PDF of $Y$ should be possible, either directly or by determining CDF and taking the derivative, to do so we however have to make sure the function is continuous over the real line.

\subsubsection{Uniform distribution}
A uniform distribution is a distribution with a PDF given by
\[
    f_{X}(x)=\begin{cases}\frac{1}{b-a} & a<x<b \\ 0 & x\notin[a,b]\end{cases}
\]
The expected value of a uniform distribution is given by
\begin{align*}
    \text{Var}(x)&=\int_{-\infty}^{\infty}xf_{X}(x)dx \\
             &=\int_{-\infty}^{\infty}x\times\left(\frac{1}{b-a}\right)dx \\
             &=\frac{1}{b-a}\times\left[\frac{x^{2}}{2}\right]_{a}^{b} \\
             &=\frac{1}{b-a}\times\left(\frac{b^{2}}{2}-\frac{a^{2}}{2}\right) \\
             &=\frac{b^{2}-a^{2}}{2(b-a)} \\
             &=\frac{b-a}{2}
\end{align*}
To determine the variance we need $EX^{2}$
\begin{align*}
    EX^{2}&=\int_{-\infty}^{\infty}x^{2}f_{X}(x)dx \\
       &=\int_{-\infty}^{\infty}x^{2}\times\left(\frac{1}{b-a}\right)dx \\
       &=\frac{1}{b-a}\times\left[\frac{x^{3}}{3}\right]_{a}^{b} \\
       &=\frac{1}{b-a}\times\left(\frac{b^{3}}{3}-\frac{a^{3}}{3}\right) \\
       &=\frac{b^{3}-a^{3}}{3(b-a)} \\
       &=\frac{(b-a)(b^{2}+a^{2}+ab)}{3(b-a)} \\
       &=\frac{b^{2}+a^{2}+ab}{3}
\end{align*}
As such we can determine the variance as
\begin{align*}
    Var(X)&=EX^{2}-(EX)^{2} \\
          &=\frac{b^{2}+a^{2}+ab}{3}-\left(\frac{b-a}{2}\right)^{2} \\
          &=\frac{b^{2}+a^{2}+ab}{3}-\frac{(b-a)^{2}}{4} \\
          &=\frac{4(b^{2}+a^{2}+ab)}{12}-\frac{3(b^{2}+a^{2}-2ab)}{12} \\
          &=\frac{4b^{2}+4a^{2}+4ab-3b^{2}-3a^{2}+6ab}{12} \\
          &=\frac{b^{2}+a^{2}-2ab}{12}
\end{align*}
\subsubsection{Exponential distribution}
An exponential distribution is a distribution with a PDF given by
\[
    f_{X}(x)=\begin{cases}\lambda e^{-\lambda x} & x>0 \\ 0 & \text{otherwise}\end{cases}
\]
The expected value of a uniform distribution is given by
\begin{align*}
    EX&=\int_{-\infty}^{\infty}xf_{X}(x)dx \\
      &=\int_{-\infty}^{\infty}x\times\lambda e^{-\lambda x}dx \\
      &=\lambda\int_{-\infty}^{\infty}xe^{-\lambda x}dx \\
\end{align*}
We now make use of integration by parts with $f=x$ and $g'=e^{-\lambda x}$
\[
    \int xe^{-\lambda x}=-\frac{xe^{-\lambda x}}{\lambda}-\int-\frac{e^{-\lambda x}}{\lambda}dx
\]
And solve the integral using substitution with $u=-\lambda x\implies dx=\frac{1}{-\lambda}du$
\begin{align*}
    \int-\frac{e^{u}}{\lambda}\frac{1}{-\lambda}du\right)&=\frac{1}{\lambda^{2}}\int e^{u}du \\
      &=\frac{e^{u}}{\lambda^{2}} \\
      &=\frac{e^{-\lambda x}}{\lambda^{2}}
\end{align*}
As such we can compute the expected value as
\begin{align*}
    EX&=\int_{-\infty}^{\infty}\lambda\times\left(-\frac{xe^{-\lambda x}}{\lambda}-\frac{e^{-\lambda x}}{\lambda^{2}}\right)dx \\
      &=\int_{-\infty}^{\infty}-xe^{-\lambda x}-\frac{e^{-\lambda x}}{\lambda}dx \\
      &=\int_{-\infty}^{\infty}-\frac{\lambda xe^{-\lambda x}-e^{-\lambda x}}{\lambda}dx \\
      &=\left[\frac{(\lambda x+1)e^{-\lambda x}}{\lambda}\right]_{0}^{\infty} \\
      &=\frac{(\lambda\times 0+1)e^{0}}{\lambda}-\lim_{x\rightarrow \infty}\frac{(\lambda x+1)e^{-\lambda x}}{\lambda} \\
      &=\frac{1}{\lambda}
\end{align*}
To determine the variance we need $EX^{2}$
\begin{align*}
    EX^{2}&=\int_{0}^{\infty}x^{2}\times \lambda e^{-\lambda x}dx \\
       &=\lambda\int_{0}^{\infty}x^{2}\times e^{-\lambda x}dx \\
       &=\lambda\int_{0}^{\infty}\left(-\frac{x^{2}e^{-\lambda x}}{\lambda}-\int-\frac{2xe^{-\lambda x}}{\lambda}\right) \\
       &=\lambda\int_{0}^{\infty}\left(-\frac{x^{2}e^{-\lambda x}}{\lambda}+\frac{2}{\lambda}\int xe^{-\lambda x}\right) \\
       &=\lambda\int_{0}^{\infty}\left(-\frac{x^{2}e^{-\lambda x}}{\lambda}+\frac{2}{\lambda}\times\left(\frac{e^{-\lambda x}}{\lambda^{2}}\right)\right) \\
       &=\int_{0}^{\infty}-x^{2}e^{-\lambda x}+\frac{2e^{-\lambda x}}{\lambda^{2}} \\
       &=\int_{0}^{\infty}\frac{-\lambda^{2}x^{2}e^{-\lambda x}+2e^{-\lambda x}}{\lambda^{2}} \\
       &=\left[\frac{-\lambda^{2}x^{2}e^{-\lambda x}+2e^{-\lambda x}}{\lambda^{2}}\right]_{0}^{\infty} \\
       &=\frac{-\lambda^{2}0^{2}e^{-\lambda 0}+2e^{0}}{\lambda^{2}}-\lim_{x\rightarrow \infty}\frac{-\lambda^{2}x^{2}e^{-\lambda x}+2e^{-\lambda x}}{\lambda^{2}} \\
       &=\frac{2}{\lambda^{2}}
\end{align*}
As such we can determine the variance as
\begin{align*}
    \text{Var}(X)&=EX^{2}-(EX)^{2} \\
             &=\frac{2}{\lambda^{2}}-\frac{1}{\lambda^{2}} \\
             &=\frac{1}{\lambda^{2}}
\end{align*}
\subsubsection{Normal (Gaussian) distribution}
A standard normal random variable is a random variable whos PDF follows
\[
    f_{Z}(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}}
\]
Where the first term ensures that the area under the curve is equal to one. The expected value of such a distribution is given by
\begin{align*}
    EZ&=\int_{-\infty}^{\infty}zf_{Z}(z)dz \\
      &=\int_{-\infty}^{\infty}z\times\frac{1}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}}dz \\
      &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}z\times e^{-\frac{z^{2}}{2}}dz 
\end{align*}
Making use of integration by substitution with $u=-\frac{z^{2}}{2}\implies dz=\frac{1}{-z}du$
\begin{align*}
    EZ&=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}z\times e^{u}\times\frac{1}{-z}du \\
      &=-\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{u}du \\
      &=-\frac{1}{\sqrt{2\pi}}\left[e^{u}\right]_{-\infty}^{\infty} \\
      &=-\frac{1}{\sqrt{2\pi}}\left[e^{-\frac{z^{2}}{2}}\right]_{-\infty}^{\infty} \\
      &=-\frac{1}{\sqrt{2\pi}}\times\left(\lim_{z\rightarrow\infty}e^{-\frac{z^{2}}{2}}-\lim_{z\rightarrow-\infty}e^{-\frac{z^{2}}{2}}\right) \\
      &=0
\end{align*}
This makes sense as the distribution is symmetrical around 0. We now determine $EZ^{2}$ to enable determining the variance.
\begin{align*}
    EZ^{2}&=\int_{-\infty}^{\infty} z^{2}f_{Z}(z)dz \\ 
          &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} z^{2}e^{-\frac{z^2}{2}}dz \\ 
          &=\frac{1}{\sqrt{2\pi}}\left(\int_{-\infty}^{0} z\left(ze^{-\frac{z^2}{2}}dz\right)+\int_{0}^{\infty} z\left(ze^{-\frac{z^2}{2}}\right)dz\right) \\ 
          &=\frac{1}{\sqrt{2\pi}}\left(\left[-ze^{-\frac{z^2}{2}}\right]_{-\infty}^{0}+\int_{-\infty}^{0}e^{-\frac{z^2}{2}}dz-\left[ze^{-\frac{z^2}{2}}\right]_{0}^{\infty}+\int_{0}^{\infty} e^{-\frac{z^2}{2}}dz\right) \\ 
          &=\frac{1}{\sqrt{2\pi}}\left(\int_{-\infty}^{0}e^{-\frac{z^2}{2}}dz+\int_{0}^{\infty} e^{-\frac{z^2}{2}}dz\right) \\
          &=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-\frac{z^2}{2}}dz \\ 
          &=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}}dz \\ 
          &=\int_{-\infty}^{\infty}f_{Z}(z)dz \\ 
          &=1 
\end{align*}
As such we can determine the variance of the Gaussian distribution as
\begin{align*}
    \text{Var}(Z)&=EZ^{2}-(EZ)^{2} \\
                 &=1-0 \\
                 &=1
\end{align*}
The CDF of the normal distribution which we would typically find by integrating the PDF, this integral does however not have a closed form solution, and as such we typically denote the CDF of a standard normal as
\[
    \phi(x)=P(Z\leq x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-\frac{x^{2}}{2}}dz
\]
Furthermore we define a normal random variable, a random variable which can be transformed into any other random variable, for this we have that
\begin{align*}
    X&=\sigma Z+\mu\text{, where }\sigma>0 \\
    EX&=\sigma EZ+\mu=\mu \\
    \text{Var}(X)&=\sigma^{2}\text{Var}(Z)=\sigma^{2}
\end{align*}
By this definition we call $X$ a normal random variable with mean $\mu$ and variance $\sigma^{2}$, notationally this can be written
\[
    X\sim N(\mu,\sigma^{2})
\]
A random variable with such distribution has that
\begin{align*}
    P(X<z)&=F_{X}(z) \\
          &=\phi\left(\frac{z-\mu}{\sqrt{\sigma}}\right)
\end{align*}
\begin{theorem}
  A linear transformation of a normal random variable is itself a normal random variable.
\end{theorem}
\begin{proof}
  Let
  \[
      X\sim N(\mu_{X},\sigma^{2}_{X})\text{ and }Y=aX+b\text{ where }a,b\in \mathbb{R}
  \]
  Expanding the shorthand we have that
  \[
      X=\sigma_{X}Z+\mu_{X}
  \]
  As such we write that
  \begin{align*}
      Y&=aX+b \\
       &=a(\sigma_{X}Z+\mu_{X})+b \\
       &=a\sigma_{X}Z+a\mu_{X}+b
  \end{align*}
  Which can also be written as
  \begin{align*}
      Y&=(a\sigma_{X})Z+(a\mu_{X}+b) \\
       &=N(a\mu_{X}+b,a^{2}\sigma_{X}^{2})
  \end{align*}
\end{proof}
\subsubsection{Gamma distribution}
The gamma distribution makes use of the gamma function, denoted by $\Gamma(x)$, which is defined by
\[
    \Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}e^{-x}dx\text{, }\alpha>0
\]
In practice, the gamma function is an extension of the factorial operation to the real (and complex) numbers. This function has some notable properties
\begin{itemize}
    \item[-] $\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}e^{-x}dx$
    \item[-] $\int_{0}^{\infty}x^{\alpha-1}e^{-\lambda x}dx=\frac{\Gamma(\alpha)}{\lambda^{\alpha}}$ for $\lambda>0$
    \item[-] $\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$
    \item[-] $\Gamma(n)=(n-1)!$ for $n\in\mathbb{N}$
    \item[-] $\Gamma(\frac{1}{2})=\sqrt{\pi}$
\end{itemize}
The gamme function has a PDF given by
\[
    f_{X}(x)=\begin{cases}\frac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha) & x>0 \\ 0 & \text{otherwise}}\end{cases}
\]
If we let $\alpha=1$ we get
\[
    f_{X}(x)\begin{cases}\lambda e^{-\lambda} & x>0 \\ 0 & \text{otherwise}\end{cases}
\]
We recognize that this is the PDF of an exponential distribution, and as such the gamma distribution can be seen as the sum of $n$ independent exponential distributions. We again determine the expected value and variance of a gamma distribution, using LOTUS we have that
\begin{align*}
    EX&=\int_{-\infty}^{\infty}xf_{X}(x) \\
      &=\int_{-\infty}^{\infty}x\frac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)} \\
      &=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha}e^{-\lambda x} \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{\Gamma(\alpha+1)}{\lambda^{\alpha+1}} \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{\alpha\Gamma(\alpha)}{\lambda^{\alpha+1}} \\
      &=\frac{\alpha}{\lambda}
\end{align*}
The same method is employed to determine $EX^{2}$, as such
\begin{align*}
    EX^{2}&=\int_{-\infty}^{\infty}x^{2}f_{X}(x)dx \\
      &=\int_{-\infty}^{\infty}x^{2}\frac{\lambda^{\alpha}x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)}dx \\
      &=\frac{\lambda^{\alpha}}{\Gamma(\alpha)}\int_{0}^{\infty}x^{\alpha+1}e^{-\lambda x}dx \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{\Gamma(\alpha+2)}{\lambda^{\alpha+2}} \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{\Gamma(\alpha+2)}{\lambda^{\alpha+2}} \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{(\alpha+1)\Gamma(\alpha+1)}{\lambda^{\alpha+2}} \\
      &=\frac{\lambda^{a}}{\Gamma(\alpha)}\times\frac{(\alpha+1)\alpha\Gamma(\alpha)}{\lambda^{\alpha+2}} \\
      &=\frac{(\alpha+1)\alpha}{\lambda^{2}} \\
      &=\frac{a^{2}+\alpha}{\lambda^{2}}
\end{align*}
We now determine the variance and get that
\begin{align*}
    \text{Var}(X)&=EX^{2}-(EX)^{2} \\
             &=\frac{a^{2}+a}{\lambda^{2}}-\frac{a^{2}}{\lambda^{2}} \\
             &=\frac{a}{\lambda^{2}}
\end{align*}
\subsection{Mixed random variables}
As opposed to the previous exclusively discrete or continuous random variables, mixed random variables feature both a discrete and continuous part. In general these can be written as the sum of a continuous and discrete function
\[
    F_{Y}(y)=C(y)+D(y)
\]
To determine the PDF of a mixed random variable, we differentiate the continuous part of the CDF
\[
    c(y)=\frac{d}{dy}C(y)\text{, where }C(y)\text{ is differentiable}
\]
This PDF is not valid as it doesn't integrate to one. As the random variable isn't continuous over the entire interval, we have to add the sum of the jump points whereby we'll have a valid PDF, as such we define that
\[
    P_{Y}(y)=\int_{-\infty}^{\infty}c(y)dy+\sum_{y_{k}}P(Y=y_{k})=1
\]
Using LOTUS we can then obtain a general formula for the expected value of $Y$ as
\[
    EY=\int_{-\infty}^{\infty}yc(y)dy+\sum_{y_{k}}y_{k}P(Y=y_{k})
\]
\subsubsection{Delta function}
The Dirac delta function serves to unify the theory of discrete, continuous and mixed random variable by allowing us the define PDFs, which was previously reserved for only the continuous random variables, for both the disceret and mixed variables.
